{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/iridhexx/miniconda3/envs/project2/lib/python3.12/site-packages/torchtext/vocab/__init__.py:4: UserWarning: \n",
      "/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n",
      "Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n",
      "  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n",
      "/home/iridhexx/miniconda3/envs/project2/lib/python3.12/site-packages/torchtext/utils.py:4: UserWarning: \n",
      "/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n",
      "Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n",
      "  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n"
     ]
    }
   ],
   "source": [
    "# IO\n",
    "import os\n",
    "import csv\n",
    "import pathlib\n",
    "from pathlib import Path\n",
    "import chardet\n",
    "import warnings\n",
    "\n",
    "# Utilities\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import copy\n",
    "\n",
    "# Preprocessing\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from torchtext.vocab import Vocab\n",
    "from collections import Counter\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "# Visualization\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Modelling and training\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "## Metrics\n",
    "from sklearn.metrics import confusion_matrix\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download `nltk` resources (only needs to run once per machine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /home/iridhexx/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/iridhexx/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt_tab')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Early stopping class (pytorch does not have a built in early stopping callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    def __init__(self, monitor='val_loss', patience=1, restore_best_weights=True, mode='min'):\n",
    "        self.monitor = monitor\n",
    "        self.patience = patience\n",
    "        self.restore_best_weights = restore_best_weights\n",
    "        self.mode = mode  # 'min' for loss, 'max' for accuracy, etc.\n",
    "        self.best_score = None\n",
    "        self.counter = 0\n",
    "        self.early_stop = False\n",
    "        self.best_model_state = None\n",
    "\n",
    "    def __call__(self, current_score, model):\n",
    "        # Determine if the current score is better\n",
    "        if self.best_score is None:\n",
    "            self.best_score = current_score\n",
    "            if self.restore_best_weights:\n",
    "                self.best_model_state = copy.deepcopy(model.state_dict())\n",
    "        elif (self.mode == 'min' and current_score < self.best_score) or \\\n",
    "             (self.mode == 'max' and current_score > self.best_score):\n",
    "            self.best_score = current_score\n",
    "            self.counter = 0\n",
    "            if self.restore_best_weights:\n",
    "                self.best_model_state = copy.deepcopy(model.state_dict())\n",
    "        else:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "\n",
    "    def restore(self, model):\n",
    "        if self.restore_best_weights and self.best_model_state:\n",
    "            model.load_state_dict(self.best_model_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LoSiento(nn.Module):\n",
    "    def __init__(self, max_features, embed_dim=128, lstm_out=196, output_classes=3):\n",
    "        super(LoSiento, self).__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(num_embeddings=max_features, embedding_dim=embed_dim)\n",
    "        self.spatial_dropout = nn.Dropout2d(p=0.5)  # Approximate SpatialDropout1D\n",
    "        self.lstm = nn.LSTM(embed_dim, lstm_out, batch_first=True, dropout=0.3)\n",
    "        \n",
    "        self.dropout_1 = nn.Dropout(p=0.2)\n",
    "        self.dense_1 = nn.Linear(lstm_out, 100)\n",
    "        self.dropout_2 = nn.Dropout(p=0.4)\n",
    "        self.output_layer = nn.Linear(100, output_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)                      # (batch, seq_len, embed_dim)\n",
    "        x = x.permute(0, 2, 1)                     # For Dropout2d: (batch, embed_dim, seq_len)\n",
    "        x = self.spatial_dropout(x)\n",
    "        x = x.permute(0, 2, 1)                     # Back to (batch, seq_len, embed_dim)\n",
    "        \n",
    "        x, _ = self.lstm(x)                        # LSTM returns (output, (h_n, c_n))\n",
    "        x = x[:, -1, :]                            # Get the output from the last timestep\n",
    "        \n",
    "        x = self.dropout_1(x)\n",
    "        x = F.relu(self.dense_1(x))\n",
    "        x = self.dropout_2(x)\n",
    "        x = self.output_layer(x)\n",
    "        return F.softmax(x, dim=1)         "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The encoding of the text file is automatically detected here with some confidence. It could be extracted from terminal using\n",
    "```\n",
    "file -I file.txt\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Terminal method (not os agnostic):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!file -i ~/Documents/StudyResources/IML/Project/Part2/_data/FinancialPhraseBank-v1.0/Sentences_50Agree.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Python method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_sentences_from_file(filepath):\n",
    "    \"\"\"Given an input txt file, extract sentences \n",
    "    and associated sentiments\n",
    "\n",
    "    Args:\n",
    "        filepath (string): path to string \n",
    "\n",
    "    Returns:\n",
    "        list: list of sentences and sentiments\n",
    "    \"\"\"\n",
    "    sentences = []\n",
    "    # automatically detect endoding (best guess)\n",
    "    with open(filepath, 'rb') as file:\n",
    "        encoding = chardet.detect(file.read())['encoding']\n",
    "    # read and split\n",
    "    with open(filepath, 'r', encoding=encoding) as file:\n",
    "        for line in file:\n",
    "            line = line.strip()\n",
    "            if '.@' in line:\n",
    "                sentence, sentiment = line.rsplit('.@', 1)\n",
    "                sentence = sentence.strip()\n",
    "                sentiment = sentiment.strip().lower()\n",
    "                sentence = fix_common_mojibake(sentence)\n",
    "                sentences.append((sentence, sentiment))\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def txt_to_csv(txt_path, output_csv):\n",
    "    \"\"\"Takes a file path as input and generates a .csv file\n",
    "    which contains sentences and sentiments as columns, extracted\n",
    "    from the .txt file corresponding to the path\n",
    "\n",
    "    Args:\n",
    "        txt_path (string): path to the .txt file\n",
    "        output_csv (string): path to desired output .csv file\n",
    "    \"\"\"\n",
    "    # Skip processing if CSV already exists\n",
    "    if os.path.exists(output_csv):\n",
    "        print(f\"{output_csv} already exists. Skipping processing.\")\n",
    "        return\n",
    "\n",
    "    sentences = extract_sentences_from_file(txt_path)\n",
    "\n",
    "    with open(output_csv, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        writer.writerow(['Sentence', 'Sentiment'])  # Header\n",
    "        writer.writerows(sentences)\n",
    "    print(f\"Processed files and wrote output to {output_csv}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function is needed as the text files seem to be corrupted, for example one sentence is:\n",
    "```\n",
    "Clothing retail chain Sepp+ñl+ñ 's sales increased by 8 % to EUR 155.2 mn , and operating profit rose to EUR 31.1 mn from EUR 17.1 mn in 2004\n",
    "```\n",
    "The characters `+ñ` are a result of choosing the wrong encoding; in fact, here the detected encoding is latin-1, and those characters correspond to `ä` in utf-8. The function below is used to fix these common mistakes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_common_mojibake(text):\n",
    "    \"\"\"Function to manually handle encoding errors\n",
    "\n",
    "    Args:\n",
    "        text (string): input sentence\n",
    "\n",
    "    Returns:\n",
    "        string: output corrected sentence\n",
    "    \"\"\"\n",
    "    replacements = {\n",
    "        '+ñ': 'ä',\n",
    "        '+í': 'é',\n",
    "        '+ô': 'ö',\n",
    "        '+ü': 'ü'\n",
    "        }\n",
    "    for wrong, right in replacements.items():\n",
    "        text = text.replace(wrong, right)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function summarizes the common preprocessing pipeline for natural language models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    # Tokenize the text\n",
    "    tokens = word_tokenize(text.lower())\n",
    "\n",
    "    # Remove stop words\n",
    "    filtered_tokens = [token for token in tokens if token not in stopwords.words('english')]\n",
    "\n",
    "    # Lemmatize the tokens\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(token) for token in filtered_tokens]\n",
    "\n",
    "    # Join the tokens back into a string\n",
    "    processed_text = ' '.join(lemmatized_tokens)\n",
    "\n",
    "    return processed_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "project2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
