{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/exterior/miniconda3/envs/project2/lib/python3.12/site-packages/torchtext/vocab/__init__.py:4: UserWarning: \n",
      "/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n",
      "Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n",
      "  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n",
      "/Users/exterior/miniconda3/envs/project2/lib/python3.12/site-packages/torchtext/utils.py:4: UserWarning: \n",
      "/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n",
      "Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n",
      "  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n",
      "/Users/exterior/miniconda3/envs/project2/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# IO\n",
    "import os\n",
    "import csv\n",
    "import pathlib\n",
    "from pathlib import Path\n",
    "import chardet\n",
    "import warnings\n",
    "\n",
    "# Utilities\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import copy\n",
    "\n",
    "# Preprocessing\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from torchtext.vocab import Vocab\n",
    "from collections import Counter\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "# Visualization\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Modelling and training\n",
    "import optuna\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "## Metrics\n",
    "from sklearn.metrics import confusion_matrix\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download `nltk` resources (only needs to run once per machine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /Users/exterior/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/exterior/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt_tab')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Early stopping class (pytorch does not have a built in early stopping callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    def __init__(self, monitor='val_loss', patience=1, restore_best_weights=True, mode='min'):\n",
    "        self.monitor = monitor\n",
    "        self.patience = patience\n",
    "        self.restore_best_weights = restore_best_weights\n",
    "        self.mode = mode  # 'min' for loss, 'max' for accuracy, etc.\n",
    "        self.best_score = None\n",
    "        self.counter = 0\n",
    "        self.early_stop = False\n",
    "        self.best_model_state = None\n",
    "\n",
    "    def __call__(self, current_score, model):\n",
    "        # Determine if the current score is better\n",
    "        if self.best_score is None:\n",
    "            self.best_score = current_score\n",
    "            if self.restore_best_weights:\n",
    "                self.best_model_state = copy.deepcopy(model.state_dict())\n",
    "        elif (self.mode == 'min' and current_score < self.best_score) or \\\n",
    "             (self.mode == 'max' and current_score > self.best_score):\n",
    "            self.best_score = current_score\n",
    "            self.counter = 0\n",
    "            if self.restore_best_weights:\n",
    "                self.best_model_state = copy.deepcopy(model.state_dict())\n",
    "        else:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "\n",
    "    def restore(self, model):\n",
    "        if self.restore_best_weights and self.best_model_state:\n",
    "            model.load_state_dict(self.best_model_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextDataset(Dataset):\n",
    "    def __init__(self, df, vocab_inst):\n",
    "        self.vocab_inst = vocab_inst\n",
    "        self.texts = df['Sentence'].tolist()\n",
    "        self.labels = df['Sentiment_encoded'].tolist()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        encoded = torch.tensor(encode_text(self.texts[idx]), dtype=torch.long)\n",
    "        label = torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        return encoded, label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LoSiento(nn.Module):\n",
    "    def __init__(self, max_features, embed_dim=128, lstm_out=196, output_classes=3):\n",
    "        super(LoSiento, self).__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(num_embeddings=max_features, embedding_dim=embed_dim)\n",
    "        self.spatial_dropout = nn.Dropout2d(p=0.5)  # Approximate SpatialDropout1D\n",
    "        self.lstm = nn.LSTM(embed_dim, lstm_out, batch_first=True, dropout=0.3)\n",
    "        \n",
    "        self.dropout_1 = nn.Dropout(p=0.2)\n",
    "        self.dense_1 = nn.Linear(lstm_out, 100)\n",
    "        self.dropout_2 = nn.Dropout(p=0.4)\n",
    "        self.output_layer = nn.Linear(100, output_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)                      # (batch, seq_len, embed_dim)\n",
    "        x = x.permute(0, 2, 1)                     # For Dropout2d: (batch, embed_dim, seq_len)\n",
    "        x = self.spatial_dropout(x)\n",
    "        x = x.permute(0, 2, 1)                     # Back to (batch, seq_len, embed_dim)\n",
    "        \n",
    "        x, _ = self.lstm(x)                        # LSTM returns (output, (h_n, c_n))\n",
    "        x = x[:, -1, :]                            # Get the output from the last timestep\n",
    "        \n",
    "        x = self.dropout_1(x)\n",
    "        x = F.relu(self.dense_1(x))\n",
    "        x = self.dropout_2(x)\n",
    "        x = self.output_layer(x)\n",
    "        return F.softmax(x, dim=1)         "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The encoding of the text file is automatically detected here with some confidence. It could be extracted from terminal using\n",
    "```\n",
    "file -I file.txt\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Terminal method (not os agnostic):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!file -i ~/Documents/StudyResources/IML/Project/Part2/_data/FinancialPhraseBank-v1.0/Sentences_50Agree.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Python method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_sentences_from_file(filepath):\n",
    "    \"\"\"Given an input txt file, extract sentences \n",
    "    and associated sentiments\n",
    "\n",
    "    Args:\n",
    "        filepath (string): path to string \n",
    "\n",
    "    Returns:\n",
    "        list: list of sentences and sentiments\n",
    "    \"\"\"\n",
    "    sentences = []\n",
    "    # automatically detect endoding (best guess)\n",
    "    with open(filepath, 'rb') as file:\n",
    "        encoding = chardet.detect(file.read())['encoding']\n",
    "    # read and split\n",
    "    with open(filepath, 'r', encoding=encoding) as file:\n",
    "        for line in file:\n",
    "            line = line.strip()\n",
    "            if '.@' in line:\n",
    "                sentence, sentiment = line.rsplit('.@', 1)\n",
    "                sentence = sentence.strip()\n",
    "                sentiment = sentiment.strip().lower()\n",
    "                sentence = fix_common_mojibake(sentence)\n",
    "                sentences.append((sentence, sentiment))\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def txt_to_csv(txt_path, output_csv):\n",
    "    \"\"\"Takes a file path as input and generates a .csv file\n",
    "    which contains sentences and sentiments as columns, extracted\n",
    "    from the .txt file corresponding to the path\n",
    "\n",
    "    Args:\n",
    "        txt_path (string): path to the .txt file\n",
    "        output_csv (string): path to desired output .csv file\n",
    "    \"\"\"\n",
    "    # Skip processing if CSV already exists\n",
    "    if os.path.exists(output_csv):\n",
    "        print(f\"{output_csv} already exists. Skipping processing.\")\n",
    "        return\n",
    "\n",
    "    sentences = extract_sentences_from_file(txt_path)\n",
    "\n",
    "    with open(output_csv, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        writer.writerow(['Sentence', 'Sentiment'])  # Header\n",
    "        writer.writerows(sentences)\n",
    "    print(f\"Processed files and wrote output to {output_csv}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function is needed as the text files seem to be corrupted, for example one sentence is:\n",
    "```\n",
    "Clothing retail chain Sepp+ñl+ñ 's sales increased by 8 % to EUR 155.2 mn , and operating profit rose to EUR 31.1 mn from EUR 17.1 mn in 2004\n",
    "```\n",
    "The characters `+ñ` are a result of choosing the wrong encoding; in fact, here the detected encoding is latin-1, and those characters correspond to `ä` in utf-8. The function below is used to fix these common mistakes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_common_mojibake(text):\n",
    "    \"\"\"Function to manually handle encoding errors\n",
    "\n",
    "    Args:\n",
    "        text (string): input sentence\n",
    "\n",
    "    Returns:\n",
    "        string: output corrected sentence\n",
    "    \"\"\"\n",
    "    replacements = {\n",
    "        '+ñ': 'ä',\n",
    "        '+í': 'é',\n",
    "        '+ô': 'ö',\n",
    "        '+ü': 'ü'\n",
    "        }\n",
    "    for wrong, right in replacements.items():\n",
    "        text = text.replace(wrong, right)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function summarizes the common preprocessing pipeline for natural language models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    # Tokenize the text\n",
    "    tokens = word_tokenize(text.lower())\n",
    "\n",
    "    # Remove stop words\n",
    "    filtered_tokens = [token for token in tokens if token not in stopwords.words('english')]\n",
    "\n",
    "    # Lemmatize the tokens\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(token) for token in filtered_tokens]\n",
    "\n",
    "    # Join the tokens back into a string\n",
    "    processed_text = ' '.join(lemmatized_tokens)\n",
    "\n",
    "    return processed_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function that creates csv files (if not created already) and returns the dataframe extracted from them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_df():\n",
    "    \"\"\"Import data and creates csv's and pandas dataframes\n",
    "\n",
    "    Returns:\n",
    "        pandas dataframes for training and testing\n",
    "    \"\"\"\n",
    "    print(\"Please select which dataset to use. Data is categorized based on the percentage of agreement in the sentiment estimator.\")\n",
    "    print(\"Options:\")\n",
    "    print(\"(1) 50\")\n",
    "    print(\"(2) 66\")\n",
    "    print(\"(3) 75\")\n",
    "    print(\"(4) 100\")\n",
    "    percentage = input()\n",
    "    if percentage=='50':\n",
    "        txt_path = RAW_DATA_FOLDER + 'Sentences_50Agree.txt'\n",
    "        csv_path = DATASET_FOLDER + 'sentences_50.csv'\n",
    "        txt_to_csv(txt_path, csv_path)\n",
    "    elif percentage=='66':\n",
    "        txt_path = RAW_DATA_FOLDER + 'Sentences_66Agree.txt'\n",
    "        csv_path = DATASET_FOLDER + 'sentences_66.csv'\n",
    "        txt_to_csv(txt_path, csv_path)\n",
    "    elif percentage=='75':\n",
    "        txt_path = RAW_DATA_FOLDER + 'Sentences_75Agree.txt'\n",
    "        csv_path = DATASET_FOLDER + 'sentences_75.csv'\n",
    "        txt_to_csv(txt_path, csv_path)\n",
    "    elif percentage=='100':\n",
    "        txt_path = RAW_DATA_FOLDER + 'Sentences_AllAgree.txt'\n",
    "        csv_path = DATASET_FOLDER + 'sentences_100.csv'\n",
    "        txt_to_csv(txt_path, csv_path)\n",
    "    else:\n",
    "        print(\"The percentage provided is not admitted, skipping.\")\n",
    "        return\n",
    "\n",
    "    df = pd.read_csv(csv_path)\n",
    "\n",
    "    le = LabelEncoder()\n",
    "    le.fit(df['Sentiment'])\n",
    "    df['Sentiment_encoded'] = le.transform(df['Sentiment'])\n",
    "\n",
    "    return df, le"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataloaders(train_df, val_df, test_df=None, vocab_inst=None):\n",
    "    if vocab_inst is None:\n",
    "        vocab_inst = build_vocab(train_df)\n",
    "\n",
    "    train_dataset = TextDataset(train_df, vocab_inst)\n",
    "    val_dataset = TextDataset(val_df, vocab_inst)\n",
    "    test_dataset = TextDataset(test_df, vocab_inst) if test_df is not None else None\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=32, collate_fn=collate_fn, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=32, collate_fn=collate_fn)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=32, collate_fn=collate_fn) if test_df is not None else None\n",
    "\n",
    "    return train_loader, val_loader, test_loader, vocab_inst\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following functions are needed to create token vocabulary and dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocab(df):\n",
    "    counter = Counter()\n",
    "    for text in df['Sentence']:\n",
    "        counter.update(preprocess_text(text))\n",
    "\n",
    "    vocab_inst = vocab(counter, specials=[\"<pad>\", \"<unk>\"])\n",
    "    vocab_inst.set_default_index(vocab_inst[\"<unk>\"])\n",
    "    return vocab_inst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_text(text, vocab_inst):\n",
    "    tokens = preprocess_text(text)\n",
    "    return [vocab_inst[token] for token in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch, vocab_inst):\n",
    "    texts, labels = zip(*batch)\n",
    "    padded_texts = pad_sequence(texts, batch_first=True, padding_value=vocab_inst[\"<pad>\"])\n",
    "    labels = torch.stack(labels)\n",
    "    return padded_texts, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(\n",
    "    trial, \n",
    "    df, \n",
    "    model_class,\n",
    "    device\n",
    "    ):\n",
    "    # Sample hyperparameters\n",
    "    hidden_size = trial.suggest_int(\"hidden_size\", 64, 256)\n",
    "    lr = trial.suggest_float(\"lr\", 1e-4, 1e-2, log=True)\n",
    "    num_layers = trial.suggest_int(\"num_layers\", 1, 3)\n",
    "\n",
    "    accuracies = []\n",
    "\n",
    "    # Cross-validation loop\n",
    "    for train_idx, val_idx in skf.split(df, df['Sentiment_encoded']):\n",
    "        train_df = df.iloc[train_idx].reset_index(drop=True)\n",
    "        val_df = df.iloc[val_idx].reset_index(drop=True)\n",
    "\n",
    "        # Build vocab only on training fold\n",
    "        vocab_inst = build_vocab(train_df)\n",
    "\n",
    "        train_loader, val_loader, _, _ = create_dataloaders(train_df, val_df, vocab_inst=vocab_inst)\n",
    "\n",
    "        # Define model (custom RNN or LSTM)\n",
    "        model = model_class(vocab_size=len(vocab_inst), hidden_size=hidden_size, num_layers=num_layers).to(device)\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "        loss_crit = nn.CrossEntropyLoss()\n",
    "\n",
    "        # Train for 1–3 epochs for speed (or early stopping)\n",
    "        for _ in range(3):\n",
    "            training_epoch(model, optimizer, loss_crit, train_loader, device)\n",
    "\n",
    "        # Evaluate on validation fold\n",
    "        model.eval()\n",
    "        correct, total = 0, 0\n",
    "        with torch.no_grad():\n",
    "            for x_val, y_val in val_loader:\n",
    "                x_val, y_val = x_val.to(device), y_val.to(device)\n",
    "                output = model(x_val)\n",
    "                pred = torch.argmax(output, dim=1)\n",
    "                correct += (pred == y_val).sum().item()\n",
    "                total += y_val.size(0)\n",
    "\n",
    "        fold_acc = correct / total\n",
    "        accuracies.append(fold_acc)\n",
    "\n",
    "    # Return average CV accuracy\n",
    "    return sum(accuracies) / len(accuracies)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define training and validation epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_epoch(model, optimizer, loss_crit, train_loader, device):\n",
    "    model.train()\n",
    "    avg_train_loss = 0\n",
    "    correct_train = 0\n",
    "    total_train = 0\n",
    "    \n",
    "    # Load batches and train\n",
    "    for batch_x, batch_y in train_loader:\n",
    "        batch_x, batch_y = batch_x.to(device), batch_y.to(device)  # Move data to device\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(batch_x)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = loss_crit(outputs, batch_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Accumulate train loss\n",
    "        avg_train_loss += loss.item() * batch_x.size(0)\n",
    "\n",
    "        # Accumulate train accuracy\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        correct_train += (predicted == batch_y).sum().item()\n",
    "        total_train += batch_y.size(0)\n",
    "    \n",
    "    # Average training loss and accuracy over batches\n",
    "    avg_train_loss /= total_train\n",
    "    avg_train_accuracy = correct_train / total_train\n",
    "\n",
    "    return avg_train_loss, avg_train_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation_epoch(model, criterion, val_loader, device):\n",
    "    model.eval()\n",
    "    avg_val_loss = 0\n",
    "    correct_val = 0\n",
    "    total_val = 0\n",
    "    \n",
    "    with torch.no_grad(): # No training\n",
    "        for batch_x, batch_y in val_loader:\n",
    "            batch_x, batch_y = batch_x.to(device), batch_y.to(device)  # Move data to device\n",
    "            outputs = model(batch_x)\n",
    "\n",
    "            # Compute loss\n",
    "            loss = criterion(outputs, batch_y)\n",
    "\n",
    "            # Accumulate train loss\n",
    "            avg_val_loss += loss.item() * batch_x.size(0)\n",
    "\n",
    "            # Accumulate validation accuracy\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            correct_val += (predicted == batch_y).sum().item()\n",
    "            total_val += batch_y.size(0)\n",
    "\n",
    "    \n",
    "    # Average validation loss for the fold\n",
    "    avg_val_loss /= total_val\n",
    "    avg_val_accuracy = correct_val / total_val\n",
    "    \n",
    "    return avg_val_loss, avg_val_accuracy"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "project2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
