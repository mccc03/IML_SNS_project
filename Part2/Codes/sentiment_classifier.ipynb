{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/exterior/miniconda3/envs/project2/lib/python3.12/site-packages/torchtext/vocab/__init__.py:4: UserWarning: \n",
      "/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n",
      "Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n",
      "  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n",
      "/Users/exterior/miniconda3/envs/project2/lib/python3.12/site-packages/torchtext/utils.py:4: UserWarning: \n",
      "/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n",
      "Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n",
      "  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n",
      "/Users/exterior/miniconda3/envs/project2/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# IO\n",
    "import os\n",
    "import csv\n",
    "import pathlib\n",
    "from pathlib import Path\n",
    "import chardet\n",
    "import warnings\n",
    "\n",
    "# Utilities\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import copy\n",
    "from functools import partial\n",
    "\n",
    "# Preprocessing\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from torchtext.vocab import vocab\n",
    "from collections import Counter\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "\n",
    "# Visualization\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Modelling and training\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import optuna\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Evaluation\n",
    "from sklearn.metrics import (\n",
    "    roc_curve, auc, roc_auc_score,\n",
    "    accuracy_score, precision_score, recall_score, \n",
    "    f1_score, confusion_matrix\n",
    ")\n",
    "from sklearn.preprocessing import label_binarize\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download `nltk` resources (only needs to run once per machine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /Users/exterior/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/exterior/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt_tab')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Early stopping class (pytorch does not have a built in early stopping callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    def __init__(self, patience=1, restore_best_weights=True, mode='min'):\n",
    "        self.patience = patience\n",
    "        self.restore_best_weights = restore_best_weights\n",
    "        self.mode = mode  # 'min' for loss, 'max' for accuracy, etc.\n",
    "        self.best_score = None\n",
    "        self.counter = 0\n",
    "        self.early_stop = False\n",
    "        self.best_model_state = None\n",
    "\n",
    "    def __call__(self, current_score, model):\n",
    "        # Determine if the current score is better\n",
    "        if self.best_score is None:\n",
    "            self.best_score = current_score\n",
    "            if self.restore_best_weights:\n",
    "                self.best_model_state = copy.deepcopy(model.state_dict())\n",
    "        elif (self.mode == 'min' and current_score < self.best_score) or \\\n",
    "             (self.mode == 'max' and current_score > self.best_score):\n",
    "            self.best_score = current_score\n",
    "            self.counter = 0\n",
    "            if self.restore_best_weights:\n",
    "                self.best_model_state = copy.deepcopy(model.state_dict())\n",
    "        else:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "\n",
    "    def restore(self, model):\n",
    "        if self.restore_best_weights and self.best_model_state:\n",
    "            model.load_state_dict(self.best_model_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextDataset(Dataset):\n",
    "    def __init__(self, df, vocab_inst):\n",
    "        self.vocab_inst = vocab_inst\n",
    "        self.texts = df['Sentence'].tolist()\n",
    "        self.labels = df['Sentiment_encoded'].tolist()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        encoded = torch.tensor(encode_text(self.texts[idx], vocab_inst), dtype=torch.long)\n",
    "        label = torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        return encoded, label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CorpoSpeakDecoder(nn.Module):\n",
    "    def __init__(\n",
    "        self, \n",
    "        vocab_size, \n",
    "        embed_dim=128, \n",
    "        lstm_out=196,\n",
    "        dropout_spatial=0.5, \n",
    "        dropout_lstm=0.3,\n",
    "        dropout_1=0.2,\n",
    "        dropout_2=0.4, \n",
    "        dense_1=100,\n",
    "        output_classes=3):\n",
    "        \n",
    "        super(CorpoSpeakDecoder, self).__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(num_embeddings=vocab_size, embedding_dim=embed_dim)\n",
    "        self.spatial_dropout = nn.Dropout2d(p=dropout_spatial)  # Approximate SpatialDropout1D\n",
    "        self.lstm = nn.LSTM(embed_dim, lstm_out, batch_first=True, dropout=dropout_lstm)\n",
    "        \n",
    "        self.dropout_1 = nn.Dropout(p=dropout_1)\n",
    "        self.dense_1 = nn.Linear(lstm_out, dense_1)\n",
    "        self.dropout_2 = nn.Dropout(p=dropout_2)\n",
    "        self.output_layer = nn.Linear(dense_1, output_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)                      # (batch, seq_len, embed_dim)\n",
    "        x = x.permute(0, 2, 1)                     # For Dropout2d: (batch, embed_dim, seq_len)\n",
    "        x = self.spatial_dropout(x)\n",
    "        x = x.permute(0, 2, 1)                     # Back to (batch, seq_len, embed_dim)\n",
    "        \n",
    "        x, _ = self.lstm(x)                        # LSTM returns (output, (h_n, c_n))\n",
    "        x = x[:, -1, :]                            # Get the output from the last timestep\n",
    "        \n",
    "        x = self.dropout_1(x)\n",
    "        x = F.relu(self.dense_1(x))\n",
    "        x = self.dropout_2(x)\n",
    "        x = self.output_layer(x)\n",
    "        return F.softmax(x, dim=1)         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleRNNClassifier(nn.Module):\n",
    "    def __init__(\n",
    "        self, \n",
    "        max_features, \n",
    "        max_length, \n",
    "        embedding_dim=16, \n",
    "        rnn_units=64, \n",
    "        num_classes=3\n",
    "        ):\n",
    "        super(SimpleRNNClassifier, self).__init__()\n",
    "        self.embedding = nn.Embedding(num_embeddings=max_features, embedding_dim=embedding_dim)\n",
    "        self.rnn = nn.RNN(input_size=embedding_dim, hidden_size=rnn_units, batch_first=True, nonlinearity='tanh')\n",
    "        self.fc = nn.Linear(rnn_units, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)                        # shape: (batch_size, max_length, embedding_dim)\n",
    "        output, hidden = self.rnn(x)                 # hidden: (1, batch_size, rnn_units)\n",
    "        x = hidden.squeeze(0)                        # shape: (batch_size, rnn_units)\n",
    "        x = self.fc(x)                               # shape: (batch_size, num_classes)\n",
    "        x = F.softmax(x, dim=1)                      # shape: (batch_size, num_classes)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The encoding of the text file is automatically detected here with some confidence. It could be extracted from terminal using\n",
    "```\n",
    "file -I file.txt\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Terminal method (not os agnostic):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!file -i ~/Documents/StudyResources/IML/Project/Part2/_data/FinancialPhraseBank-v1.0/Sentences_50Agree.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Python method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_sentences_from_file(filepath):\n",
    "    \"\"\"Given an input txt file, extract sentences \n",
    "    and associated sentiments\n",
    "\n",
    "    Args:\n",
    "        filepath (string): path to string \n",
    "\n",
    "    Returns:\n",
    "        list: list of sentences and sentiments\n",
    "    \"\"\"\n",
    "    sentences = []\n",
    "    # automatically detect endoding (best guess)\n",
    "    with open(filepath, 'rb') as file:\n",
    "        encoding = chardet.detect(file.read())['encoding']\n",
    "    # read and split\n",
    "    with open(filepath, 'r', encoding=encoding) as file:\n",
    "        for line in file:\n",
    "            line = line.strip()\n",
    "            if '.@' in line:\n",
    "                sentence, sentiment = line.rsplit('.@', 1)\n",
    "                sentence = sentence.strip()\n",
    "                sentiment = sentiment.strip().lower()\n",
    "                sentence = fix_common_mojibake(sentence)\n",
    "                sentences.append((sentence, sentiment))\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def txt_to_csv(txt_path, output_csv):\n",
    "    \"\"\"Takes a file path as input and generates a .csv file\n",
    "    which contains sentences and sentiments as columns, extracted\n",
    "    from the .txt file corresponding to the path\n",
    "\n",
    "    Args:\n",
    "        txt_path (string): path to the .txt file\n",
    "        output_csv (string): path to desired output .csv file\n",
    "    \"\"\"\n",
    "    # Skip processing if CSV already exists\n",
    "    if os.path.exists(output_csv):\n",
    "        print(f\"{output_csv} already exists. Skipping processing.\")\n",
    "        return\n",
    "\n",
    "    sentences = extract_sentences_from_file(txt_path)\n",
    "\n",
    "    with open(output_csv, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        writer.writerow(['Sentence', 'Sentiment'])  # Header\n",
    "        writer.writerows(sentences)\n",
    "    print(f\"Processed files and wrote output to {output_csv}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function is needed as the text files seem to be corrupted, for example one sentence is:\n",
    "```\n",
    "Clothing retail chain Sepp+Ã±l+Ã± 's sales increased by 8 % to EUR 155.2 mn , and operating profit rose to EUR 31.1 mn from EUR 17.1 mn in 2004\n",
    "```\n",
    "The characters `+Ã±` are a result of choosing the wrong encoding; in fact, here the detected encoding is latin-1, and those characters correspond to `Ã¤` in utf-8. The function below is used to fix these common mistakes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_common_mojibake(text):\n",
    "    \"\"\"Function to manually handle encoding errors\n",
    "\n",
    "    Args:\n",
    "        text (string): input sentence\n",
    "\n",
    "    Returns:\n",
    "        string: output corrected sentence\n",
    "    \"\"\"\n",
    "    replacements = {\n",
    "        '+Ã±': 'Ã¤',\n",
    "        '+Ã­': 'Ã©',\n",
    "        '+Ã´': 'Ã¶',\n",
    "        '+Ã¼': 'Ã¼'\n",
    "        }\n",
    "    for wrong, right in replacements.items():\n",
    "        text = text.replace(wrong, right)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function summarizes the common preprocessing pipeline for natural language models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    # Tokenize the text\n",
    "    tokens = word_tokenize(text.lower())\n",
    "\n",
    "    # Remove stop words\n",
    "    filtered_tokens = [token for token in tokens if token not in stopwords.words('english')]\n",
    "\n",
    "    # Lemmatize the tokens\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(token) for token in filtered_tokens]\n",
    "\n",
    "    # Join the tokens back into a string\n",
    "    processed_text = ' '.join(lemmatized_tokens)\n",
    "\n",
    "    return processed_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function that creates csv files (if not created already) and returns the dataframe extracted from them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_df():\n",
    "    \"\"\"Import data and creates csv's and pandas dataframes\n",
    "\n",
    "    Returns:\n",
    "        pandas dataframes for training and testing\n",
    "    \"\"\"\n",
    "    print(\"Please select which dataset to use. Data is categorized based on the percentage of agreement in the sentiment estimator.\")\n",
    "    print(\"Options:\")\n",
    "    print(\"(1) 50\")\n",
    "    print(\"(2) 66\")\n",
    "    print(\"(3) 75\")\n",
    "    print(\"(4) 100\")\n",
    "    percentage = input()\n",
    "    if percentage=='50':\n",
    "        txt_path = RAW_DATA_FOLDER + 'Sentences_50Agree.txt'\n",
    "        csv_path = DATASET_FOLDER + 'sentences_50.csv'\n",
    "        txt_to_csv(txt_path, csv_path)\n",
    "    elif percentage=='66':\n",
    "        txt_path = RAW_DATA_FOLDER + 'Sentences_66Agree.txt'\n",
    "        csv_path = DATASET_FOLDER + 'sentences_66.csv'\n",
    "        txt_to_csv(txt_path, csv_path)\n",
    "    elif percentage=='75':\n",
    "        txt_path = RAW_DATA_FOLDER + 'Sentences_75Agree.txt'\n",
    "        csv_path = DATASET_FOLDER + 'sentences_75.csv'\n",
    "        txt_to_csv(txt_path, csv_path)\n",
    "    elif percentage=='100':\n",
    "        txt_path = RAW_DATA_FOLDER + 'Sentences_AllAgree.txt'\n",
    "        csv_path = DATASET_FOLDER + 'sentences_100.csv'\n",
    "        txt_to_csv(txt_path, csv_path)\n",
    "    else:\n",
    "        print(\"The percentage provided is not admitted, skipping.\")\n",
    "        return\n",
    "\n",
    "    df = pd.read_csv(csv_path)\n",
    "\n",
    "    le = LabelEncoder()\n",
    "    le.fit(df['Sentiment'])\n",
    "    df['Sentiment_encoded'] = le.transform(df['Sentiment'])\n",
    "\n",
    "    return df, le"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataloaders(train_df, val_df, vocab_inst=None):\n",
    "    if vocab_inst is None:\n",
    "        vocab_inst = build_vocab(train_df)\n",
    "\n",
    "    train_dataset = TextDataset(train_df, vocab_inst)\n",
    "    val_dataset = TextDataset(val_df, vocab_inst)\n",
    "\n",
    "    collate_with_vocab = partial(collate_fn, vocab_inst=vocab_inst)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=32, collate_fn=collate_with_vocab, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=32, collate_fn=collate_with_vocab)\n",
    "\n",
    "    return train_loader, val_loader, vocab_inst\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following functions are needed to create token vocabulary and dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocab(df):\n",
    "    counter = Counter()\n",
    "    for text in df['Sentence']:\n",
    "        counter.update(preprocess_text(text))\n",
    "\n",
    "    vocab_inst = vocab(counter, specials=[\"<pad>\", \"<unk>\"])\n",
    "    vocab_inst.set_default_index(vocab_inst[\"<unk>\"])\n",
    "    return vocab_inst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_text(text, vocab_inst):\n",
    "    tokens = preprocess_text(text)\n",
    "    return [vocab_inst[token] for token in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch, vocab_inst):\n",
    "    texts, labels = zip(*batch)\n",
    "    padded_texts = pad_sequence(texts, batch_first=True, padding_value=vocab_inst[\"<pad>\"])\n",
    "    labels = torch.stack(labels)\n",
    "    return padded_texts, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO\n",
    "Maybe it's better to feed it the dictionary generated outside of the objective function?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(\n",
    "    trial, \n",
    "    df,\n",
    "    vocab_inst,\n",
    "    cv,\n",
    "    device\n",
    "    ):\n",
    "    # Sample hyperparameters\n",
    "    embed_dim = trial.suggest_int(\"embed_dim\", 64, 256, step=32)\n",
    "    lr = trial.suggest_float(\"lr\", 1e-4, 1e-2, log=True)\n",
    "    wd = trial.suggest_float(\"weight_decay\", 1e-4, 1e-2, log=True)\n",
    "    lstm_out = trial.suggest_int(\"lstm_out\", 64, 256, step=32)\n",
    "    dropout_spatial = trial.suggest_float(\"dropout_spatial\", 0.1, 0.5, step=0.2)\n",
    "    dropout_lstm = trial.suggest_float(\"dropout_lstm\", 0.1, 0.5, step=0.2)\n",
    "    dropout_1 = trial.suggest_float(\"dropout_1\", 0.1, 0.5, step=0.2)\n",
    "    dropout_2 = trial.suggest_float(\"dropout_2\", 0.1, 0.5, step=0.2)\n",
    "    dense_1 = trial.suggest_int(\"dense_1\", 64, 256, step=64)\n",
    "\n",
    "    accuracies = []\n",
    "\n",
    "    # Cross-validation loop\n",
    "    for train_idx, val_idx in cv.split(df, df['Sentiment_encoded']):\n",
    "\n",
    "        early_stop = EarlyStopping(patience=3, mode='max')\n",
    "\n",
    "        train_df = df.iloc[train_idx].reset_index(drop=True)\n",
    "        val_df = df.iloc[val_idx].reset_index(drop=True)\n",
    "\n",
    "        # Build vocab only on training fold\n",
    "        vocab_size = len(vocab_inst)\n",
    "\n",
    "        train_loader, val_loader, _ = create_dataloaders(train_df, val_df, vocab_inst=vocab_inst)\n",
    "\n",
    "        # Define model (custom RNN or LSTM)\n",
    "        model = CorpoSpeakDecoder(\n",
    "            vocab_size, \n",
    "            embed_dim=embed_dim, \n",
    "            lstm_out=lstm_out,\n",
    "            dropout_spatial=dropout_spatial,\n",
    "            dropout_lstm=dropout_lstm, \n",
    "            dropout_1=dropout_1,\n",
    "            dropout_2=dropout_2,\n",
    "            dense_1=dense_1\n",
    "            )\n",
    "        model = model.to(device)\n",
    "\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=wd)\n",
    "        loss_crit = nn.CrossEntropyLoss().to(device)\n",
    "\n",
    "        # Train for 5 epochs for speed (or early stopping)\n",
    "        for _ in range(5):\n",
    "            _, _ = training_epoch(model, optimizer, loss_crit, train_loader, device)\n",
    "            val_loss, val_acc = validation_epoch(model, loss_crit, val_loader, device)\n",
    "\n",
    "            early_stop(val_acc, model)\n",
    "            if early_stop.early_stop:\n",
    "                print(\"Early stopping triggered.\")\n",
    "                break\n",
    "\n",
    "        early_stop.restore(model)\n",
    "        \n",
    "        # Evaluate on validation fold\n",
    "        model.eval()\n",
    "        correct, total = 0, 0\n",
    "        with torch.no_grad():\n",
    "            for x_val, y_val in val_loader:\n",
    "                x_val, y_val = x_val.to(device), y_val.to(device)\n",
    "                output = model(x_val)\n",
    "                pred = torch.argmax(output, dim=1)\n",
    "                correct += (pred == y_val).sum().item()\n",
    "                total += y_val.size(0)\n",
    "\n",
    "        fold_acc = correct / total\n",
    "        accuracies.append(fold_acc)\n",
    "\n",
    "    # Return average CV accuracy\n",
    "    return sum(accuracies) / len(accuracies)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define training and validation epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_epoch(model, optimizer, loss_crit, train_loader, device):\n",
    "    model.train()\n",
    "    avg_train_loss = 0\n",
    "    correct_train = 0\n",
    "    total_train = 0\n",
    "    \n",
    "    # Load batches and train\n",
    "    for batch_x, batch_y in train_loader:\n",
    "        batch_x, batch_y = batch_x.to(device), batch_y.to(device)  # Move data to device\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(batch_x)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = loss_crit(outputs, batch_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Accumulate train loss\n",
    "        avg_train_loss += loss.item() * batch_x.size(0)\n",
    "\n",
    "        # Accumulate train accuracy\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        correct_train += (predicted == batch_y).sum().item()\n",
    "        total_train += batch_y.size(0)\n",
    "    \n",
    "    # Average training loss and accuracy over batches\n",
    "    avg_train_loss /= total_train\n",
    "    avg_train_accuracy = correct_train / total_train\n",
    "\n",
    "    return avg_train_loss, avg_train_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation_epoch(model, criterion, val_loader, device):\n",
    "    model.eval()\n",
    "    avg_val_loss = 0\n",
    "    correct_val = 0\n",
    "    total_val = 0\n",
    "    \n",
    "    with torch.no_grad(): # No training\n",
    "        for batch_x, batch_y in val_loader:\n",
    "            batch_x, batch_y = batch_x.to(device), batch_y.to(device)  # Move data to device\n",
    "            outputs = model(batch_x)\n",
    "\n",
    "            # Compute loss\n",
    "            loss = criterion(outputs, batch_y)\n",
    "\n",
    "            # Accumulate train loss\n",
    "            avg_val_loss += loss.item() * batch_x.size(0)\n",
    "\n",
    "            # Accumulate validation accuracy\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            correct_val += (predicted == batch_y).sum().item()\n",
    "            total_val += batch_y.size(0)\n",
    "\n",
    "    \n",
    "    # Average validation loss for the fold\n",
    "    avg_val_loss /= total_val\n",
    "    avg_val_accuracy = correct_val / total_val\n",
    "    \n",
    "    return avg_val_loss, avg_val_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(\n",
    "    model,\n",
    "    df_train,\n",
    "    vocab_inst,\n",
    "    optimizer,\n",
    "    loss_crit,\n",
    "    device,\n",
    "    hyperparameters,\n",
    "    num_epochs=10,\n",
    "    early_stopping=None\n",
    "    ):\n",
    "\n",
    "    train_df, val_df = train_test_split(\n",
    "        df_train,\n",
    "        test_size=val_size,\n",
    "        stratify=train_val_df[\"Sentiment_encoded\"],\n",
    "        random_state=42\n",
    "        )\n",
    "\n",
    "    train_loader, val_loader, vocab_inst = create_dataloaders(train_df, val_df, vocab_inst=vocab_inst)\n",
    "\n",
    "    history = {\n",
    "        \"train_loss\": [],\n",
    "        \"train_acc\": [],\n",
    "        \"val_loss\": [],\n",
    "        \"val_acc\": []\n",
    "        }\n",
    "\n",
    "    early_stop = EarlyStopping(mode='max')\n",
    "\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        print(f\"Epoch {epoch}/{num_epochs}\")\n",
    "\n",
    "        # Training step\n",
    "        train_loss, train_acc = training_epoch(model, optimizer, loss_crit, train_loader, device)\n",
    "\n",
    "        # Validation step\n",
    "        val_loss, val_acc = validation_epoch(model, loss_crit, val_loader, device)\n",
    "\n",
    "        # Log metrics\n",
    "        history[\"train_loss\"].append(train_loss)\n",
    "        history[\"train_acc\"].append(train_acc)\n",
    "        history[\"val_loss\"].append(val_loss)\n",
    "        history[\"val_acc\"].append(val_acc)\n",
    "\n",
    "        print(f\"Train Loss: {train_loss:.4f}, Acc: {train_acc:.4f} | Val Loss: {val_loss:.4f}, Acc: {val_acc:.4f}\")\n",
    "\n",
    "        early_stop(val_acc, model)\n",
    "        if early_stop.early_stop:\n",
    "            early_stop.restore(model)\n",
    "            print(\"Early stopping triggered.\")\n",
    "            break\n",
    "\n",
    "    return model, history\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model_sklearn(model, dataloader, num_classes, device='cpu'):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_probs = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            inputs, labels = batch\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            probs = outputs.detach().cpu().numpy()\n",
    "            preds = np.argmax(probs, axis=1)\n",
    "\n",
    "            all_probs.extend(probs)\n",
    "            all_preds.extend(preds)\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    all_probs = np.array(all_probs)\n",
    "    all_preds = np.array(all_preds)\n",
    "    all_labels = np.array(all_labels)\n",
    "\n",
    "    print(\"ðŸ“Š Evaluation Results\")\n",
    "    print(\"-------------------------\")\n",
    "    print(f\"Accuracy         : {accuracy_score(all_labels, all_preds):.4f}\")\n",
    "    print(f\"Precision (macro): {precision_score(all_labels, all_preds, average='macro'):.4f}\")\n",
    "    print(f\"Recall    (macro): {recall_score(all_labels, all_preds, average='macro'):.4f}\")\n",
    "    print(f\"F1 Score  (macro): {f1_score(all_labels, all_preds, average='macro'):.4f}\")\n",
    "    print(\"\\nConfusion Matrix:\")\n",
    "    print(confusion_matrix(all_labels, all_preds))\n",
    "\n",
    "    # Binarize the labels for ROC computation\n",
    "    y_true_bin = label_binarize(all_labels, classes=np.arange(num_classes))\n",
    "\n",
    "    # Compute ROC curve and AUC for each class\n",
    "    fpr = dict()\n",
    "    tpr = dict()\n",
    "    roc_auc = dict()\n",
    "\n",
    "    for i in range(num_classes):\n",
    "        fpr[i], tpr[i], _ = roc_curve(y_true_bin[:, i], all_probs[:, i])\n",
    "        roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "    # Compute macro-average ROC curve and AUC\n",
    "    all_fpr = np.unique(np.concatenate([fpr[i] for i in range(num_classes)]))\n",
    "    mean_tpr = np.zeros_like(all_fpr)\n",
    "\n",
    "    for i in range(num_classes):\n",
    "        mean_tpr += np.interp(all_fpr, fpr[i], tpr[i])\n",
    "\n",
    "    mean_tpr /= num_classes\n",
    "    fpr[\"macro\"] = all_fpr\n",
    "    tpr[\"macro\"] = mean_tpr\n",
    "    roc_auc[\"macro\"] = auc(fpr[\"macro\"], tpr[\"macro\"])\n",
    "\n",
    "    print(f\"\\nROC-AUC (macro-average): {roc_auc['macro']:.4f}\")\n",
    "\n",
    "    # Plot all ROC curves\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    for i in range(num_classes):\n",
    "        plt.plot(fpr[i], tpr[i], label=f\"Class {i} (AUC = {roc_auc[i]:.2f})\")\n",
    "    plt.plot(fpr[\"macro\"], tpr[\"macro\"], label=f\"Macro Avg (AUC = {roc_auc['macro']:.2f})\", \n",
    "             linestyle='--', color='navy')\n",
    "    plt.plot([0, 1], [0, 1], 'k--', lw=1)\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Multiclass ROC Curves')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO multiple folds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import data and create dataframes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OS-agnostic working folder and data folder definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "CodeDirectory = Path(os.path.abspath(''))\n",
    "DATASET_FOLDER = os.path.join(str(CodeDirectory.parent.absolute()), \"_data\",\"\")\n",
    "RAW_DATA_FOLDER = os.path.join(str(DATASET_FOLDER), \"FinancialPhraseBank-v1.0\",\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_csv_path = DATASET_FOLDER+'data.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import data into pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please select which dataset to use. Data is categorized based on the percentage of agreement in the sentiment estimator.\n",
      "Options:\n",
      "(1) 50\n",
      "(2) 66\n",
      "(3) 75\n",
      "(4) 100\n",
      "/Users/exterior/Documents/IML/Project/Part2/_data/sentences_50.csv already exists. Skipping processing.\n"
     ]
    }
   ],
   "source": [
    "df, le = create_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 4730 entries, 0 to 4729\n",
      "Data columns (total 3 columns):\n",
      " #   Column             Non-Null Count  Dtype \n",
      "---  ------             --------------  ----- \n",
      " 0   Sentence           4730 non-null   object\n",
      " 1   Sentiment          4730 non-null   object\n",
      " 2   Sentiment_encoded  4730 non-null   int64 \n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 111.0+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split data into testing and training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train, df_test = train_test_split(\n",
    "    df,\n",
    "    test_size=0.2,\n",
    "    stratify=df[\"Sentiment_encoded\"],\n",
    "    random_state=42\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminary analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_inst = build_vocab(df_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "skf = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyperparameter search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-13 16:24:24,526] A new study created in memory with name: no-name-0b4e238a-5ebd-4d32-b952-f7d02b46d074\n",
      "/Users/exterior/miniconda3/envs/project2/lib/python3.12/site-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "/Users/exterior/miniconda3/envs/project2/lib/python3.12/site-packages/torch/nn/functional.py:1381: UserWarning: dropout2d: Received a 3D input to dropout2d and assuming that channel-wise 1D dropout behavior is desired - input is interpreted as shape (N, C, L), where C is the channel dim. This behavior will change in a future release to interpret the input as one without a batch dimension, i.e. shape (C, H, W). To maintain the 1D channel-wise dropout behavior, please switch to using dropout1d instead.\n",
      "  warnings.warn(\"dropout2d: Received a 3D input to dropout2d and assuming that channel-wise \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping triggered.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/exterior/miniconda3/envs/project2/lib/python3.12/site-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping triggered.\n",
      "Early stopping triggered.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-13 16:42:42,650] Trial 0 finished with value: 0.5953488010283184 and parameters: {'embed_dim': 64, 'lr': 0.0019348024511993806, 'weight_decay': 0.005692081828030703, 'lstm_out': 224, 'dropout_spatial': 0.30000000000000004, 'dropout_lstm': 0.5, 'dropout_1': 0.30000000000000004, 'dropout_2': 0.1, 'dense_1': 128}. Best is trial 0 with value: 0.5953488010283184.\n",
      "/Users/exterior/miniconda3/envs/project2/lib/python3.12/site-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "/Users/exterior/miniconda3/envs/project2/lib/python3.12/site-packages/torch/nn/functional.py:1381: UserWarning: dropout2d: Received a 3D input to dropout2d and assuming that channel-wise 1D dropout behavior is desired - input is interpreted as shape (N, C, L), where C is the channel dim. This behavior will change in a future release to interpret the input as one without a batch dimension, i.e. shape (C, H, W). To maintain the 1D channel-wise dropout behavior, please switch to using dropout1d instead.\n",
      "  warnings.warn(\"dropout2d: Received a 3D input to dropout2d and assuming that channel-wise \"\n",
      "[W 2025-06-13 16:48:39,495] Trial 1 failed with parameters: {'embed_dim': 192, 'lr': 0.00018266315967171545, 'weight_decay': 0.009353119888868408, 'lstm_out': 224, 'dropout_spatial': 0.5, 'dropout_lstm': 0.1, 'dropout_1': 0.1, 'dropout_2': 0.5, 'dense_1': 64} because of the following error: KeyboardInterrupt().\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/exterior/miniconda3/envs/project2/lib/python3.12/site-packages/optuna/study/_optimize.py\", line 197, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "                      ^^^^^^^^^^^\n",
      "  File \"/var/folders/lp/7swnz8p52ml3gfmyschj9p4m0000gn/T/ipykernel_2799/704218771.py\", line 52, in objective\n",
      "    _, _ = training_epoch(model, optimizer, loss_crit, train_loader, device)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/var/folders/lp/7swnz8p52ml3gfmyschj9p4m0000gn/T/ipykernel_2799/3756641744.py\", line 15, in training_epoch\n",
      "    loss.backward()\n",
      "  File \"/Users/exterior/miniconda3/envs/project2/lib/python3.12/site-packages/torch/_tensor.py\", line 525, in backward\n",
      "    torch.autograd.backward(\n",
      "  File \"/Users/exterior/miniconda3/envs/project2/lib/python3.12/site-packages/torch/autograd/__init__.py\", line 267, in backward\n",
      "    _engine_run_backward(\n",
      "  File \"/Users/exterior/miniconda3/envs/project2/lib/python3.12/site-packages/torch/autograd/graph.py\", line 744, in _engine_run_backward\n",
      "    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n",
      "[W 2025-06-13 16:48:39,500] Trial 1 failed with value None.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[30]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m study = optuna.create_study(direction=\u001b[33m\"\u001b[39m\u001b[33mmaximize\u001b[39m\u001b[33m\"\u001b[39m)  \u001b[38;5;66;03m# You want to maximize accuracy\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[43mstudy\u001b[49m\u001b[43m.\u001b[49m\u001b[43moptimize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpartial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobjective\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdf\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvocab_inst\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvocab_inst\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcv\u001b[49m\u001b[43m=\u001b[49m\u001b[43mskf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m    \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Number of trials to try\u001b[39;49;00m\n\u001b[32m      5\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/project2/lib/python3.12/site-packages/optuna/study/study.py:475\u001b[39m, in \u001b[36mStudy.optimize\u001b[39m\u001b[34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[39m\n\u001b[32m    373\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34moptimize\u001b[39m(\n\u001b[32m    374\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    375\u001b[39m     func: ObjectiveFuncType,\n\u001b[32m   (...)\u001b[39m\u001b[32m    382\u001b[39m     show_progress_bar: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m    383\u001b[39m ) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    384\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[32m    385\u001b[39m \n\u001b[32m    386\u001b[39m \u001b[33;03m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    473\u001b[39m \u001b[33;03m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[32m    474\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m475\u001b[39m     \u001b[43m_optimize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    476\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    477\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    478\u001b[39m \u001b[43m        \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    479\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    480\u001b[39m \u001b[43m        \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    481\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mIterable\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    482\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    483\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    484\u001b[39m \u001b[43m        \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m=\u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    485\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/project2/lib/python3.12/site-packages/optuna/study/_optimize.py:63\u001b[39m, in \u001b[36m_optimize\u001b[39m\u001b[34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[39m\n\u001b[32m     61\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     62\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m n_jobs == \u001b[32m1\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m63\u001b[39m         \u001b[43m_optimize_sequential\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     64\u001b[39m \u001b[43m            \u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     65\u001b[39m \u001b[43m            \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     66\u001b[39m \u001b[43m            \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     67\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     68\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     69\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     70\u001b[39m \u001b[43m            \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     71\u001b[39m \u001b[43m            \u001b[49m\u001b[43mreseed_sampler_rng\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     72\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtime_start\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     73\u001b[39m \u001b[43m            \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     74\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     75\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     76\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m n_jobs == -\u001b[32m1\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/project2/lib/python3.12/site-packages/optuna/study/_optimize.py:160\u001b[39m, in \u001b[36m_optimize_sequential\u001b[39m\u001b[34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[39m\n\u001b[32m    157\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m    159\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m160\u001b[39m     frozen_trial = \u001b[43m_run_trial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    161\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    162\u001b[39m     \u001b[38;5;66;03m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[32m    163\u001b[39m     \u001b[38;5;66;03m# environments (e.g., services that use computing containers such as GitHub Actions).\u001b[39;00m\n\u001b[32m    164\u001b[39m     \u001b[38;5;66;03m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[32m    165\u001b[39m     \u001b[38;5;66;03m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[32m    166\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m gc_after_trial:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/project2/lib/python3.12/site-packages/optuna/study/_optimize.py:248\u001b[39m, in \u001b[36m_run_trial\u001b[39m\u001b[34m(study, func, catch)\u001b[39m\n\u001b[32m    241\u001b[39m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[33m\"\u001b[39m\u001b[33mShould not reach.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    243\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    244\u001b[39m     frozen_trial.state == TrialState.FAIL\n\u001b[32m    245\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m func_err \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    246\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(func_err, catch)\n\u001b[32m    247\u001b[39m ):\n\u001b[32m--> \u001b[39m\u001b[32m248\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m func_err\n\u001b[32m    249\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m frozen_trial\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/project2/lib/python3.12/site-packages/optuna/study/_optimize.py:197\u001b[39m, in \u001b[36m_run_trial\u001b[39m\u001b[34m(study, func, catch)\u001b[39m\n\u001b[32m    195\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m get_heartbeat_thread(trial._trial_id, study._storage):\n\u001b[32m    196\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m197\u001b[39m         value_or_values = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    198\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m exceptions.TrialPruned \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    199\u001b[39m         \u001b[38;5;66;03m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[32m    200\u001b[39m         state = TrialState.PRUNED\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 52\u001b[39m, in \u001b[36mobjective\u001b[39m\u001b[34m(trial, df, vocab_inst, cv, device)\u001b[39m\n\u001b[32m     50\u001b[39m \u001b[38;5;66;03m# Train for 5 epochs for speed (or early stopping)\u001b[39;00m\n\u001b[32m     51\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m5\u001b[39m):\n\u001b[32m---> \u001b[39m\u001b[32m52\u001b[39m     _, _ = \u001b[43mtraining_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_crit\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     53\u001b[39m     val_loss, val_acc = validation_epoch(model, loss_crit, val_loader, device)\n\u001b[32m     55\u001b[39m     early_stop(val_acc, model)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 15\u001b[39m, in \u001b[36mtraining_epoch\u001b[39m\u001b[34m(model, optimizer, loss_crit, train_loader, device)\u001b[39m\n\u001b[32m     13\u001b[39m \u001b[38;5;66;03m# Compute loss\u001b[39;00m\n\u001b[32m     14\u001b[39m loss = loss_crit(outputs, batch_y)\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     16\u001b[39m optimizer.step()\n\u001b[32m     18\u001b[39m \u001b[38;5;66;03m# Accumulate train loss\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/project2/lib/python3.12/site-packages/torch/_tensor.py:525\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    515\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    516\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    517\u001b[39m         Tensor.backward,\n\u001b[32m    518\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    523\u001b[39m         inputs=inputs,\n\u001b[32m    524\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m525\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    526\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    527\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/project2/lib/python3.12/site-packages/torch/autograd/__init__.py:267\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    262\u001b[39m     retain_graph = create_graph\n\u001b[32m    264\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    265\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    266\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m267\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    268\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    269\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    270\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    271\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    272\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    273\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    274\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    275\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/project2/lib/python3.12/site-packages/torch/autograd/graph.py:744\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    742\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    743\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m744\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    745\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    746\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    747\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    748\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "study = optuna.create_study(direction=\"maximize\")  # You want to maximize accuracy\n",
    "study.optimize(\n",
    "    partial(objective, df=df, vocab_inst=vocab_inst, cv=skf, device=device),\n",
    "    n_trials=5,  # Number of trials to try\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "project2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
